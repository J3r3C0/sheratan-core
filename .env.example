# LLM Configuration for Worker
# Uncomment and configure one of the following:

# Option 1: LM Studio (local)
# SHERATAN_LLM_BASE_URL=http://host.docker.internal:1234/v1/chat/completions
# SHERATAN_LLM_MODEL=your-model-name
# SHERATAN_LLM_API_KEY=

# Option 2: Ollama (local)
# SHERATAN_LLM_BASE_URL=http://host.docker.internal:11434/v1/chat/completions
# SHERATAN_LLM_MODEL=llama2
# SHERATAN_LLM_API_KEY=

# Option 3: OpenAI
# SHERATAN_LLM_BASE_URL=https://api.openai.com/v1/chat/completions
# SHERATAN_LLM_MODEL=gpt-4
# SHERATAN_LLM_API_KEY=sk-...

# If no LLM is configured, worker will use static fallback plan
