# ğŸ—ï¸ Sheratan Core v2 â€“ Architecture Overview

## ğŸ“Š System Architecture Diagram

```mermaid
graph TB
    subgraph "External"
        User[ğŸ‘¤ User/Dashboard]
        Browser[ğŸŒ Chrome Debug<br/>Port 9222]
    end

    subgraph "Sheratan Core System"
        subgraph "Backend Service :8000"
            BackendAPI[Backend API<br/>FastAPI]
            MissionSvc[Mission Service]
            Metrics[Metrics Stream<br/>WebSocket]
        end

        subgraph "Core Service :8001"
            CoreAPI[Core API<br/>FastAPI]
            Storage[(File Storage<br/>data/)]
            Bridge[WebRelay Bridge]
            LCP[LCP Interpreter]
        end

        subgraph "Worker Service"
            WorkerLoop[Worker Loop]
            Tools[Tool Handlers<br/>list_files<br/>read_file<br/>write_file<br/>rewrite_file]
            LLMClient[LLM Client]
        end

        subgraph "WebRelay Service :3000"
            RelayAPI[WebRelay API]
            Puppeteer[Puppeteer<br/>Browser Control]
        end

        subgraph "Shared Volumes"
            RelayOut[ğŸ“ relay-out/<br/>Job Queue]
            RelayIn[ğŸ“ relay-in/<br/>Result Queue]
            Project[ğŸ“ project/<br/>Workspace]
        end
    end

    subgraph "External LLM"
        LLM[ğŸ¤– LLM Provider<br/>OpenAI/Ollama/LM Studio]
    end

    %% User interactions
    User -->|HTTP| BackendAPI
    User -->|HTTP| CoreAPI

    %% Backend to Core
    BackendAPI -->|Create Mission/Task/Job| CoreAPI
    MissionSvc -->|HTTP :8001| CoreAPI

    %% Core internal flow
    CoreAPI --> Storage
    CoreAPI --> Bridge
    Bridge -->|Dispatch Job| RelayOut
    Bridge -->|Read Result| RelayIn
    CoreAPI --> LCP
    LCP -->|Create Followup Jobs| CoreAPI

    %% Worker flow
    WorkerLoop -->|Poll Jobs| RelayOut
    WorkerLoop --> Tools
    WorkerLoop -->|Write Result| RelayIn
    Tools -->|File Operations| Project
    LLMClient -->|HTTP Call| LLM
    WorkerLoop --> LLMClient

    %% WebRelay flow
    RelayAPI -->|Read Jobs| RelayOut
    RelayAPI -->|Write Results| RelayIn
    RelayAPI --> Puppeteer
    Puppeteer -->|CDP| Browser
    Browser -->|ChatGPT etc.| LLM

    %% Metrics
    CoreAPI -->|Log Calls| BackendAPI
    WorkerLoop -.->|Auto-sync| CoreAPI
    Metrics -->|WebSocket| User

    style CoreAPI fill:#4a90e2,stroke:#2e5c8a,color:#fff
    style BackendAPI fill:#50c878,stroke:#2d7a4a,color:#fff
    style WorkerLoop fill:#ffa500,stroke:#cc8400,color:#fff
    style RelayAPI fill:#9b59b6,stroke:#6c3483,color:#fff
    style LCP fill:#e74c3c,stroke:#a93226,color:#fff
    style LLM fill:#f39c12,stroke:#b87a0d,color:#fff
```

## ğŸ”Œ API Ports & Endpoints

### ğŸ“ Port 8000 â€“ Backend Service

**Container:** `sheratan_backend`  
**Base URL:** `http://localhost:8000`

| Endpoint | Method | Description | File |
|----------|--------|-------------|------|
| `/` | GET | Health check | `backend/main.py` |
| `/missions` | GET | List all missions | `backend/main.py` |
| `/missions/start` | POST | Create & start mission with task | `backend/main.py` |
| `/missions/{mission_id}/status` | GET | Aggregated mission status (sync) | `backend/main.py` |
| `/missions/{mission_id}/apply_plan/{job_id}` | POST | Apply agent plan followups | `backend/main.py` |
| `/api/missions/standard-code-analysis` | POST | One-click code analysis mission | `backend/main.py` |
| `/metrics/module-calls` | POST | Record module call metrics | `backend/metrics_stream.py` |
| `/metrics/stream` | WebSocket | Real-time metrics stream | `backend/metrics_stream.py` |

**Environment Variables:**
- `UVICORN_HOST=0.0.0.0`
- `UVICORN_PORT=8000`
- `SHERATAN_CORE_URL=http://core:8001`

---

### ğŸ“ Port 8001 â€“ Core Service (mapped from :8000)

**Container:** `sheratan_core`  
**Base URL:** `http://localhost:8001`  
**Internal:** `http://core:8000`

#### Missions API

| Endpoint | Method | Description | File |
|----------|--------|-------------|------|
| `/api/missions` | POST | Create mission | `core/sheratan_core_v2/main.py:65` |
| `/api/missions` | GET | List all missions | `core/sheratan_core_v2/main.py:72` |
| `/api/missions/{mission_id}` | GET | Get mission details | `core/sheratan_core_v2/main.py:77` |

#### Tasks API

| Endpoint | Method | Description | File |
|----------|--------|-------------|------|
| `/api/missions/{mission_id}/tasks` | POST | Create task for mission | `core/sheratan_core_v2/main.py:89` |
| `/api/tasks` | GET | List all tasks | `core/sheratan_core_v2/main.py:100` |
| `/api/tasks/{task_id}` | GET | Get task details | `core/sheratan_core_v2/main.py:105` |

#### Jobs API

| Endpoint | Method | Description | File |
|----------|--------|-------------|------|
| `/api/tasks/{task_id}/jobs` | POST | Create job for task | `core/sheratan_core_v2/main.py:117` |
| `/api/jobs` | GET | List all jobs | `core/sheratan_core_v2/main.py:128` |
| `/api/jobs/{job_id}` | GET | Get job details | `core/sheratan_core_v2/main.py:133` |
| `/api/jobs/{job_id}/dispatch` | POST | **Dispatch job to worker** | `core/sheratan_core_v2/main.py:145` |
| `/api/jobs/{job_id}/sync` | POST | **Sync result + LCP followups** | `core/sheratan_core_v2/main.py:205` |

#### System API

| Endpoint | Method | Description | File |
|----------|--------|-------------|------|
| `/` | GET | Root status | `core/sheratan_core_v2/main.py:264` |
| `/api/status` | GET | System status | `core/sheratan_core_v2/main.py:255` |
| `/static/*` | GET | Static files (dashboards) | `core/sheratan_core_v2/main.py:45` |

**Environment Variables:**
- `SHERATAN_METRICS_URL=http://backend:8000/metrics/module-calls`

**Volumes:**
- `core-data:/app/data` â€“ Persistent storage
- `relay-out:/app/webrelay_out` â€“ Job queue (outgoing)
- `relay-in:/app/webrelay_in` â€“ Result queue (incoming)

---

### ğŸ“ Port 3000 â€“ WebRelay Service

**Container:** `sheratan_webrelay`  
**Base URL:** `http://localhost:3000` (host network mode)

| Endpoint | Method | Description | File |
|----------|--------|-------------|------|
| `/api/relay` | POST | Send prompt to browser | `webrelay/src/relay-handler.ts` |
| `/health` | GET | Health check | `webrelay/src/index.ts` |

**Environment Variables:**
- `BROWSER_URL=http://localhost:9222` â€“ Chrome Debug Port
- `WEB_INTERFACE_URL=https://chatgpt.com`
- `PORT=3000`

**Volumes:**
- `relay-out:/app/webrelay_out` â€“ Shared job queue
- `relay-in:/app/webrelay_in` â€“ Shared result queue

---

### ğŸ”„ Worker Service (No HTTP Port)

**Container:** `sheratan_worker`  
**Mode:** File-based queue polling

**Supported Tool Kinds:**

| Kind | Description | File | Handler |
|------|-------------|------|---------|
| `list_files` | List files matching patterns | `worker/worker_loop.py:14` | `list_files_from_params()` |
| `read_file` | Read file content | `worker/worker_loop.py:93` | `read_file_from_params()` |
| `write_file` | Write/append to file | `worker/worker_loop.py:131` | `write_file_from_params()` |
| `rewrite_file` | Overwrite file completely | `worker/worker_loop.py:184` | `rewrite_file_from_params()` |
| `llm_call` | Generic LLM call | `worker/worker_loop.py:371` | `call_llm_generic()` |
| `agent_plan` | LLM-based planning | `worker/worker_loop.py:664` | `run_agent_plan()` |

**Environment Variables:**
- `SHERATAN_CORE_URL=http://core:8000` â€“ For auto-sync callbacks
- `SHERATAN_LLM_BASE_URL` â€“ LLM provider URL (optional)
- `SHERATAN_LLM_MODEL=gpt-4-mini` â€“ Model name
- `SHERATAN_LLM_API_KEY` â€“ API key (if required)

**Volumes:**
- `relay-out:/webrelay_out` â€“ Polls for `*.job.json` files
- `relay-in:/webrelay_in` â€“ Writes `*.result.json` files
- `./project:/workspace/project` â€“ Workspace files

---

## ğŸ”„ Self-Loop & LCP Flow

### Job Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. User creates Mission via Backend or Core                â”‚
â”‚     POST /api/missions                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Mission Service creates Tasks                           â”‚
â”‚     POST /api/missions/{id}/tasks                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Task creates Jobs                                       â”‚
â”‚     POST /api/tasks/{id}/jobs                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Job Dispatch to Worker                                  â”‚
â”‚     POST /api/jobs/{id}/dispatch                            â”‚
â”‚     â†’ Writes {job_id}.job.json to relay-out/               â”‚
â”‚     â†’ Starts background polling thread                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Worker processes Job                                    â”‚
â”‚     â€¢ Polls relay-out/ for *.job.json                       â”‚
â”‚     â€¢ Executes tool (list_files, read_file, llm_call, etc.) â”‚
â”‚     â€¢ Writes {job_id}.result.json to relay-in/              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Core syncs Result (auto or manual)                      â”‚
â”‚     POST /api/jobs/{id}/sync                                â”‚
â”‚     â€¢ Reads result from relay-in/                           â”‚
â”‚     â€¢ Updates Job in storage                                â”‚
â”‚     â€¢ Passes result to LCP Interpreter                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. LCP Interpreter processes Result                        â”‚
â”‚     lcp_actions.handle_job_result()                         â”‚
â”‚                                                              â”‚
â”‚     IF result.action == "create_followup_jobs":             â”‚
â”‚       â†’ Creates new Jobs from result.new_jobs[]             â”‚
â”‚       â†’ Auto-dispatches if auto_dispatch == true            â”‚
â”‚       â†’ AUTONOMOUS LOOP ACTIVATED âœ¨                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LCP (Language Control Protocol) Format

**Job Response Format:**

```json
{
  "ok": true,
  "action": "create_followup_jobs",
  "commentary": "Brief explanation of what was done",
  "new_jobs": [
    {
      "name": "Human-readable job name",
      "description": "Optional detailed description",
      "kind": "list_files | read_file | write_file | rewrite_file | llm_call",
      "params": {
        "root": "/workspace/project",
        "patterns": ["**/*.py"]
      },
      "auto_dispatch": true
    }
  ]
}
```

**Supported Actions:**
- `create_followup_jobs` â†’ Creates new jobs automatically
- `text_result` â†’ Stores plain text summary
- `error` â†’ Reports error condition

**LCP Integration Files:**
- `core/sheratan_core_v2/lcp_actions.py` â€“ Action interpreter
- `core/sheratan_core_v2/webrelay_bridge.py` â€“ Queue bridge
- `worker/worker_loop.py:371-661` â€“ LLM call handler with LCP parsing

---

## ğŸ“‚ File Structure Reference

### Core Service Files

```
core/
â”œâ”€â”€ sheratan_core_v2/
â”‚   â”œâ”€â”€ main.py              # FastAPI app, all API endpoints
â”‚   â”œâ”€â”€ models.py            # Pydantic models (Mission, Task, Job)
â”‚   â”œâ”€â”€ storage.py           # File-based persistence
â”‚   â”œâ”€â”€ webrelay_bridge.py   # Job queue management
â”‚   â”œâ”€â”€ lcp_actions.py       # LCP interpreter & followup creation
â”‚   â””â”€â”€ metrics_client.py    # Metrics logging
â”œâ”€â”€ data/                    # Persistent storage (missions, tasks, jobs)
â””â”€â”€ Dockerfile
```

### Backend Service Files

```
backend/
â”œâ”€â”€ main.py                      # Backend API endpoints
â”œâ”€â”€ metrics_stream.py            # WebSocket metrics streaming
â””â”€â”€ sheratan_core_adapter/
    â””â”€â”€ mission_service.py       # Mission orchestration logic
```

### Worker Service Files

```
worker/
â”œâ”€â”€ worker_loop.py               # Main worker loop & tool handlers
â”‚   â”œâ”€â”€ list_files_from_params() # Line 14-70
â”‚   â”œâ”€â”€ read_file_from_params()  # Line 93-128
â”‚   â”œâ”€â”€ write_file_from_params() # Line 131-181
â”‚   â”œâ”€â”€ rewrite_file_from_params() # Line 184-226
â”‚   â”œâ”€â”€ call_llm_generic()       # Line 371-661 (LCP support)
â”‚   â”œâ”€â”€ run_agent_plan()         # Line 664-704
â”‚   â””â”€â”€ handle_job()             # Line 707-775
â””â”€â”€ Dockerfile
```

### WebRelay Service Files

```
webrelay/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts             # Main server
â”‚   â”œâ”€â”€ relay-handler.ts     # Browser automation handler
â”‚   â””â”€â”€ puppeteer-setup.ts   # Puppeteer initialization
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ lcp_system.md        # LCP system prompt
â””â”€â”€ Dockerfile
```

---

## ğŸ¯ Key Integration Points

### 1. Core â†’ Worker (Dispatch)

**File:** `core/sheratan_core_v2/main.py:145-198`  
**Method:** File-based queue via `webrelay_bridge.enqueue_job()`  
**Writes to:** `relay-out/{job_id}.job.json`  
**Auto-sync:** Background thread polls for results (60s timeout)

### 2. Worker â†’ Core (Result)

**File:** `worker/worker_loop.py:777-834`  
**Method:** File-based queue, writes result file  
**Writes to:** `relay-in/{job_id}.result.json`  
**Sync trigger:** Core auto-sync thread OR manual POST `/api/jobs/{id}/sync`

### 3. Core â†’ LCP Interpreter (Followups)

**File:** `core/sheratan_core_v2/lcp_actions.py`  
**Trigger:** After job sync in `main.py:245`  
**Actions:**
- Parses `result.action`
- Creates new Jobs if `action == "create_followup_jobs"`
- Auto-dispatches if `auto_dispatch == true`

### 4. Backend â†’ Core (Orchestration)

**File:** `backend/sheratan_core_adapter/mission_service.py`  
**Uses:** HTTP calls to Core API (`SHERATAN_CORE_URL`)  
**Flow:**
- Creates Mission
- Creates Task
- Creates Job
- Dispatches Job
- Polls for status

---

## ğŸ”§ Configuration Files

### Docker Compose

**File:** `docker-compose.yml`

| Service | Image Port | Host Port | CPU | RAM |
|---------|-----------|-----------|-----|-----|
| backend | 8000 | 8000 | 0.5 | 256M |
| core | 8000 | 8001 | 1.0 | 512M |
| worker | - | - | 0.5 | 256M |
| webrelay | 3000 | 3000 | - | - |

### Environment Variables

**File:** `.env` (based on `.env.example`)

```bash
# LLM Configuration (used by Worker)
SHERATAN_LLM_BASE_URL=<LLM endpoint>
SHERATAN_LLM_MODEL=gpt-4-mini
SHERATAN_LLM_API_KEY=<optional>

# Options:
# - LM Studio: http://host.docker.internal:1234/v1/chat/completions
# - Ollama: http://host.docker.internal:11434/v1/chat/completions  
# - OpenAI: https://api.openai.com/v1/chat/completions
# - WebRelay: http://host.docker.internal:3000/api/relay
```

---

## ğŸš€ Quick Reference

### Start the System

```bash
cd 2_sheratan_core
docker-compose up --build
```

### Create a Mission (via Backend)

```bash
curl -X POST http://localhost:8000/missions/start \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test",
    "title": "Analyze Project",
    "description": "List and analyze all Python files",
    "project_root": "/workspace/project"
  }'
```

### Create a Mission (via Core directly)

```bash
curl -X POST http://localhost:8001/api/missions \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Code Analysis",
    "description": "Analyze codebase"
  }'
```

### Check Mission Status

```bash
curl http://localhost:8000/missions/{mission_id}/status
```

### Monitor Metrics (WebSocket)

```javascript
const ws = new WebSocket('ws://localhost:8000/metrics/stream');
ws.onmessage = (event) => console.log(JSON.parse(event.data));
```

---

**Last Updated:** December 2025  
**Version:** 2.0.0
